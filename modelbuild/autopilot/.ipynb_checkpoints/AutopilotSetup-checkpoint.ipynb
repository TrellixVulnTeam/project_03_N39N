{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with Amazon SageMaker Autopilot (Parquet input)\n",
    "\n",
    "This is the accompanying notebook for the blog post [Run AutoML experiments with large parquet datasets using Amazon SageMaker Autopilot](https://aws.amazon.com/blogs/machine-learning/run-automl-experiments-with-large-parquet-datasets-using-amazon-sagemaker-autopilot/). The example here is almost the same as [Regression with Amazon SageMaker XGBoost algorithm (Parquet)](../introduction_to_amazon_algorithms/xgboost_abalone/xgboost_parquet_input_training.ipynb).\n",
    "\n",
    "This notebook tackles the exact same problem with the same solution, but has been modified for a Parquet input to be trained in SageMaker Autopilot. The original notebook provides details of dataset and the machine learning use-case.\n",
    "\n",
    "This notebook was tested in Amazon SageMaker Studio on a ml.t3.medium instance with Python 3 (Data Science) kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (1.17.70)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.24.3-py3-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.5/132.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3) (0.10.0)\n",
      "Collecting botocore<1.28.0,>=1.27.3\n",
      "  Downloading botocore-1.27.3-py3-none-any.whl (8.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.28.0,>=1.27.3->boto3) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.28.0,>=1.27.3->boto3) (1.26.9)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.3->boto3) (1.14.0)\n",
      "Installing collected packages: botocore, s3transfer, boto3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.20.112\n",
      "    Uninstalling botocore-1.20.112:\n",
      "      Successfully uninstalled botocore-1.20.112\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.4.2\n",
      "    Uninstalling s3transfer-0.4.2:\n",
      "      Successfully uninstalled s3transfer-0.4.2\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.17.70\n",
      "    Uninstalling boto3-1.17.70:\n",
      "      Successfully uninstalled boto3-1.17.70\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awswrangler 2.2.0 requires pandas<1.2.0,>=1.1.0, but you have pandas 1.3.5 which is incompatible.\n",
      "awscli 1.23.2 requires botocore==1.25.2, but you have botocore 1.27.3 which is incompatible.\n",
      "awscli 1.23.2 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0 which is incompatible.\n",
      "awscli 1.23.2 requires rsa<4.8,>=3.1.2, but you have rsa 4.8 which is incompatible.\n",
      "awscli 1.23.2 requires s3transfer<0.6.0,>=0.5.0, but you have s3transfer 0.6.0 which is incompatible.\n",
      "aiobotocore 2.2.0 requires botocore<1.24.22,>=1.24.21, but you have botocore 1.27.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.24.3 botocore-1.27.3 s3transfer-0.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"sagemaker/DEMO-automl-parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [PyArrow](https://arrow.apache.org/docs/python/) library to store the Abalone dataset in the Parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 83.8 ms, sys: 3.29 ms, total: 87.1 ms\n",
      "Wall time: 242 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "# Download the dataset and load into a pandas dataframe\n",
    "FILE_NAME = \"abalone.csv\"\n",
    "s3.download_file(\"sagemaker-sample-files\", f\"datasets/tabular/uci_abalone/abalone.csv\", FILE_NAME)\n",
    "\n",
    "feature_names = [\n",
    "    \"Sex\",\n",
    "    \"Length\",\n",
    "    \"Diameter\",\n",
    "    \"Height\",\n",
    "    \"Whole weight\",\n",
    "    \"Shucked weight\",\n",
    "    \"Viscera weight\",\n",
    "    \"Shell weight\",\n",
    "    \"Rings\",\n",
    "]\n",
    "data = pd.read_csv(FILE_NAME, header=None, names=feature_names)\n",
    "\n",
    "data.to_parquet(\"abalone.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 149 ms, sys: 0 ns, total: 149 ms\n",
      "Wall time: 255 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-627175950948/sagemaker/DEMO-automl-parquet/abalone.parquet'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sagemaker.Session().upload_data(\"abalone.parquet\", bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting the parameters, we kick off training, and poll for status until training is completed, which in this example, takes under 1 hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML job: autopilot-parquet-06-07-16-08\n",
      "InProgress - Starting\n",
      "InProgress - AnalyzingData\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "job_name = \"autopilot-parquet-\" + strftime(\"%m-%d-%H-%M\", gmtime())\n",
    "print(\"AutoML job:\", job_name)\n",
    "\n",
    "create_auto_ml_job_params = {\n",
    "    \"AutoMLJobConfig\": {\n",
    "        \"CompletionCriteria\": {\n",
    "            \"MaxCandidates\": 5,\n",
    "        }\n",
    "    },\n",
    "    \"AutoMLJobName\": job_name,\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ContentType\": \"x-application/vnd.amazon+parquet\",\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": f\"s3://{bucket}/{prefix}/abalone.parquet\",\n",
    "                }\n",
    "            },\n",
    "            \"TargetAttributeName\": \"Rings\",\n",
    "        }\n",
    "    ],\n",
    "    \"OutputDataConfig\": {\"S3OutputPath\": f\"s3://{bucket}/{prefix}/output\"},\n",
    "    \"RoleArn\": role,\n",
    "}\n",
    "\n",
    "client = boto3.client(\"sagemaker\", region_name=region)\n",
    "client.create_auto_ml_job(**create_auto_ml_job_params)\n",
    "\n",
    "response = client.describe_auto_ml_job(AutoMLJobName=job_name)\n",
    "status = response[\"AutoMLJobStatus\"]\n",
    "secondary_status = response[\"AutoMLJobSecondaryStatus\"]\n",
    "print(f\"{status} - {secondary_status}\")\n",
    "\n",
    "while status != \"Completed\" and secondary_status != \"Failed\":\n",
    "    time.sleep(60)\n",
    "    response = client.describe_auto_ml_job(AutoMLJobName=job_name)\n",
    "    status = response[\"AutoMLJobStatus\"]\n",
    "    secondary_status = response[\"AutoMLJobSecondaryStatus\"]\n",
    "    print(f\"{status} - {secondary_status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to other Autopilot example notebooks such as [Direct Marketing with Amazon SageMaker Autopilot](sagemaker_autopilot_direct_marketing.ipynb) and [Customer Churn Prediction with Amazon SageMaker Autopilot](autopilot_customer_churn.ipynb) to see how to investigate details of each training, deploy the best candidate and run inference."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
